{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74840bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "from config import TELEGRAM_TOKEN  # –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à —Ç–æ–∫–µ–Ω –≤ config.py\n",
    "\n",
    "# –í–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pymorphy3 as pm\n",
    "\n",
    "morph = pm.MorphAnalyzer()\n",
    "normalized = pd.read_csv('normalized.csv')\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5))\n",
    "tfidf_matrix = vectorizer.fit_transform(normalized['overview'])\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –±–æ—Ç–∞\n",
    "bot = telebot.TeleBot(TELEGRAM_TOKEN)\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def send_welcome(message):\n",
    "    bot.reply_to(message, \"–ü—Ä–∏–≤–µ—Ç! –ù–∞–ø–∏—à–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∏–ª—å–º–∞, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—á–µ—à—å –Ω–∞–π—Ç–∏, –∏ —è –ø–æ–¥–±–µ—Ä—É –ø–æ—Ö–æ–∂–∏–µ.\")\n",
    "\n",
    "@bot.message_handler(func=lambda message: True)\n",
    "def find_similar_movies(message):\n",
    "    try:\n",
    "        user_query = message.text\n",
    "        s = ''\n",
    "\n",
    "        STOP_WORDS = {'–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞',\n",
    "    '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ',\n",
    "    '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞',\n",
    "    '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ',\n",
    "    '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π',\n",
    "    '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º',\n",
    "    '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç',\n",
    "    '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º',\n",
    "    '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞',\n",
    "    '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å',\n",
    "    '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è',\n",
    "    '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥',\n",
    "    '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ',\n",
    "    '–≤—Å—é', '–º–µ–∂–¥—É', '—Ñ–∏–ª—å–º'}\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "        for i in user_query.split():\n",
    "            if i.lower() not in STOP_WORDS:  # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "                s += morph.parse(i)[0].normal_form + ' '\n",
    "\n",
    "        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∏–ª—å–º–æ–≤\n",
    "        query_vector = vectorizer.transform([s])\n",
    "        cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "        similarity_scores = cosine_similarities[0]\n",
    "\n",
    "        non_zero_indices = np.where(similarity_scores > 0.1)[0]\n",
    "        non_zero_scores = similarity_scores[non_zero_indices]\n",
    "\n",
    "        if len(non_zero_indices) > 0:\n",
    "            sorted_indices = non_zero_indices[np.argsort(non_zero_scores)[::-1]]\n",
    "            top_indices = sorted_indices[:5]\n",
    "            top_scores = similarity_scores[top_indices]\n",
    "\n",
    "            response = f\"üé¨ –ó–∞–ø—Ä–æ—Å: '{user_query}'\\n\\n\"\n",
    "            response += f\"–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ñ–∏–ª—å–º—ã (–Ω–∞–π–¥–µ–Ω–æ {len(top_indices)}):\\n\\n\"\n",
    "\n",
    "            for i, (idx, score) in enumerate(zip(top_indices, top_scores), 1):\n",
    "                best_match_title = normalized.loc[idx, 'movie']\n",
    "                response += f\"{i}. '{best_match_title}' (—Å—Ö–æ–¥—Å—Ç–≤–æ: {score:.4f})\\n\"\n",
    "\n",
    "        else:\n",
    "            response = \"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∏–ª—å–º–æ–≤ —Å –Ω–µ–Ω—É–ª–µ–≤—ã–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º.\"\n",
    "\n",
    "        bot.reply_to(message, response)\n",
    "\n",
    "    except Exception as e:\n",
    "        bot.reply_to(message, f\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...\")\n",
    "    bot.polling()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
