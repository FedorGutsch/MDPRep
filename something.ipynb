{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b20da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотеки импортированы, функция нормализации готова.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pymorphy3\n",
    "import fasttext\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "import compress_fasttext\n",
    "import requests\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Инициализация морфолизатора\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "# Функция нормализации текста (определяем тут, чтобы не зависеть от внешних файлов)\n",
    "def normalize_string(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = text.split()\n",
    "    # Лемматизация\n",
    "    res = [morph.parse(word)[0].normal_form for word in words]\n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "\n",
    "# Словарь для конвертации тегов pymorphy в теги Word2Vec (Universal Dependencies)\n",
    "morph_map = {\n",
    "    'ADJF': 'ADJ', 'ADJS': 'ADJ', 'COMP': 'ADJ',\n",
    "    'INFN': 'VERB', 'PRTF': 'ADJ', 'PRTS': 'ADJ', 'GRND': 'VERB',\n",
    "    'NUMR': 'NUM', 'ADVB': 'ADV', 'NPRO': 'PRON',\n",
    "    'PRED': 'ADV', 'PREP': 'ADP', 'CONJ': 'CCONJ'\n",
    "}\n",
    "\n",
    "def normalize_and_tag(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = text.split()\n",
    "    \n",
    "    tagged_words = []\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        lemma = p.normal_form\n",
    "        pos = p.tag.POS\n",
    "        \n",
    "        # Маппинг тегов\n",
    "        ud_pos = morph_map.get(pos, pos)\n",
    "        if ud_pos is None: ud_pos = 'NOUN' # Fallback\n",
    "        \n",
    "        # Формат: слово_ТЕГ (фильм_NOUN)\n",
    "        tagged_words.append(f\"{lemma}_{ud_pos}\")\n",
    "    return tagged_words\n",
    "\n",
    "print(\"Библиотеки импортированы, функция нормализации готова.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b251a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено фильмов: 2000\n",
      "Загружено тестов: 20\n",
      "Создание TF-IDF матрицы...\n",
      "TF-IDF готов.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "try:\n",
    "    real_data = pd.read_csv('normalized.csv')\n",
    "    test_data = pd.read_csv('harderTest.csv')\n",
    "    print(f\"Загружено фильмов: {real_data.shape[0]}\")\n",
    "    print(f\"Загружено тестов: {test_data.shape[0]}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Ошибка: Файлы normalized.csv или harderTest.csv не найдены!\")\n",
    "\n",
    "# Заполнение пропусков\n",
    "real_data['overview'] = real_data['overview'].fillna('')\n",
    "\n",
    "# 1. TF-IDF Векторизация\n",
    "print(\"Создание TF-IDF матрицы...\")\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5))\n",
    "tfidf_matrix = vectorizer.fit_transform(real_data['overview'])\n",
    "print(\"TF-IDF готов.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf55094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовка текстов для Word2Vec...\n",
      "Обучение Word2Vec (локально)...\n",
      "Word2Vec обучен!\n",
      "Векторизация базы фильмов через Word2Vec...\n",
      "Точность Hybrid (Local Word2Vec * TF-IDF) @1: 0.45\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка данных для Word2Vec (список списков слов)\n",
    "print(\"Подготовка текстов для Word2Vec...\")\n",
    "# Нормализуем все описания\n",
    "sentences = [normalize_string(text).split() for text in real_data['overview']]\n",
    "\n",
    "# 2. Обучение модели\n",
    "print(\"Обучение Word2Vec (локально)...\")\n",
    "# vector_size=100 - размер вектора\n",
    "# window=5 - окно контекста\n",
    "# min_count=1 - учитывать даже слова, встречающиеся 1 раз (важно для маленьких датасетов)\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"Word2Vec обучен!\")\n",
    "\n",
    "# Сохраним модель, чтобы бот мог её подхватить\n",
    "w2v_model.save(\"local_word2vec.model\")\n",
    "\n",
    "# 3. Функция векторизации предложения (усредение векторов слов)\n",
    "def get_w2v_vector(text):\n",
    "    words = normalize_string(text).split()\n",
    "    if not words: return np.zeros(100)\n",
    "    \n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in w2v_model.wv:\n",
    "            vecs.append(w2v_model.wv[word])\n",
    "    \n",
    "    if not vecs: return np.zeros(100)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# 4. Векторизация всей базы\n",
    "print(\"Векторизация базы фильмов через Word2Vec...\")\n",
    "w2v_vectors = np.array([get_w2v_vector(t) for t in real_data['overview']])\n",
    "\n",
    "# 5. Гибридный поиск (TF-IDF * Word2Vec)\n",
    "def search_hybrid(query, top_k=1):\n",
    "    norm = normalize_string(query)\n",
    "    \n",
    "    # TF-IDF сходство\n",
    "    q_tfidf = vectorizer.transform([norm])\n",
    "    sim_tfidf = cosine_similarity(q_tfidf, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Word2Vec сходство\n",
    "    q_w2v = get_w2v_vector(norm).reshape(1, -1)\n",
    "    sim_w2v = cosine_similarity(q_w2v, w2v_vectors).flatten()\n",
    "    \n",
    "    # Гибрид\n",
    "    hybrid_score = sim_tfidf * sim_w2v\n",
    "    \n",
    "    return real_data.iloc[hybrid_score.argsort()[::-1][:top_k]]['movie'].values\n",
    "\n",
    "# Оценка точности\n",
    "correct = 0\n",
    "for i, row in test_data.iterrows():\n",
    "    target = normalize_string(row['relevant_movie']).strip()\n",
    "    preds = search_hybrid(row['query'], top_k=1)\n",
    "    if target in [normalize_string(p).strip() for p in preds]:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Точность Hybrid (Local Word2Vec * TF-IDF) @1: {correct / len(test_data):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e087c8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка E5 (Small)...\n",
      "Кодирование базы через E5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 63/63 [2:09:54<00:00, 123.72s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность E5 Vector Search @1: 0.50\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузка E5 (Small)...\")\n",
    "e5_model = SentenceTransformer('intfloat/multilingual-e5-small')\n",
    "\n",
    "# Префиксы для E5 обязательны!\n",
    "passages = [\"passage: \" + normalize_string(t) for t in real_data['overview']]\n",
    "print(\"Кодирование базы через E5...\")\n",
    "e5_embeddings = e5_model.encode(passages, normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "def search_e5(query, top_k=1):\n",
    "    q_text = \"query: \" + normalize_string(query)\n",
    "    q_emb = e5_model.encode([q_text], normalize_embeddings=True)\n",
    "    scores = cosine_similarity(q_emb, e5_embeddings).flatten()\n",
    "    return real_data.iloc[scores.argsort()[::-1][:top_k]]['movie'].values\n",
    "\n",
    "# Оценка\n",
    "correct_e5 = 0\n",
    "for i, row in test_data.iterrows():\n",
    "    target = normalize_string(row['relevant_movie']).strip()\n",
    "    preds = search_e5(row['query'], top_k=1)\n",
    "    if target in [normalize_string(p).strip() for p in preds]:\n",
    "        correct_e5 += 1\n",
    "\n",
    "print(f\"Точность E5 Vector Search @1: {correct_e5 / len(test_data):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87e24eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель уже скачана.\n",
      "Загрузка модели в память (gensim)...\n",
      "Модель Word2Vec загружена!\n",
      "Векторизация базы фильмов...\n",
      "Точность Hybrid (Pretrained W2V * TF-IDF) @1: 0.40\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "# Ссылка на модель (RusVectores: GeoWAC, CBOW, 300 dim)\n",
    "MODEL_URL = \"http://vectors.nlpl.eu/repository/20/180.zip\"\n",
    "ARCHIVE_FILE = \"180.zip\"\n",
    "MODEL_FILE = \"model.bin\" # Внутри архива он так называется\n",
    "\n",
    "\n",
    "def normalize_simple(text):\n",
    "    return ' '.join([x.split('_')[0] for x in normalize_and_tag(text)])\n",
    "\n",
    "def download_and_extract_w2v():\n",
    "    if not os.path.exists(MODEL_FILE):\n",
    "        print(\"Скачивание Word2Vec (около 600 Мб)...\")\n",
    "        r = requests.get(MODEL_URL, stream=True)\n",
    "        with open(ARCHIVE_FILE, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                if chunk: f.write(chunk)\n",
    "        \n",
    "        print(\"Распаковка архива...\")\n",
    "        with zipfile.ZipFile(ARCHIVE_FILE, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        \n",
    "        print(\"Готово.\")\n",
    "    else:\n",
    "        print(\"Модель уже скачана.\")\n",
    "\n",
    "download_and_extract_w2v()\n",
    "\n",
    "print(\"Загрузка модели в память (gensim)...\")\n",
    "# load_word2vec_format загружает стандартные бинарные файлы w2v\n",
    "w2v_model = KeyedVectors.load_word2vec_format(MODEL_FILE, binary=True)\n",
    "print(\"Модель Word2Vec загружена!\")\n",
    "\n",
    "# Векторизация\n",
    "def get_w2v_vector(text):\n",
    "    tagged_words = normalize_and_tag(text)\n",
    "    if not tagged_words: return np.zeros(300)\n",
    "    \n",
    "    vecs = []\n",
    "    for word in tagged_words:\n",
    "        if word in w2v_model:\n",
    "            vecs.append(w2v_model[word])\n",
    "        else:\n",
    "            # Fallback: пробуем без тега или с другим тегом\n",
    "            simple_word = word.split('_')[0]\n",
    "            if f\"{simple_word}_NOUN\" in w2v_model:\n",
    "                vecs.append(w2v_model[f\"{simple_word}_NOUN\"])\n",
    "                \n",
    "    if not vecs: return np.zeros(300)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "print(\"Векторизация базы фильмов...\")\n",
    "w2v_vectors = np.array([get_w2v_vector(t) for t in real_data['overview']])\n",
    "\n",
    "# Гибридный поиск\n",
    "def search_hybrid(query, top_k=1):\n",
    "    norm_simple = normalize_simple(query)\n",
    "    \n",
    "    # 1. TF-IDF\n",
    "    q_tfidf = vectorizer.transform([norm_simple])\n",
    "    sim_tfidf = cosine_similarity(q_tfidf, tfidf_matrix).flatten()\n",
    "    \n",
    "    # 2. Word2Vec\n",
    "    q_w2v = get_w2v_vector(query).reshape(1, -1)\n",
    "    sim_w2v = cosine_similarity(q_w2v, w2v_vectors).flatten()\n",
    "    \n",
    "    # 3. Hybrid\n",
    "    hybrid_score = sim_tfidf * sim_w2v\n",
    "    return real_data.iloc[hybrid_score.argsort()[::-1][:top_k]]['movie'].values\n",
    "\n",
    "# Оценка\n",
    "correct = 0\n",
    "for i, row in test_data.iterrows():\n",
    "    target = normalize_simple(row['relevant_movie']).strip()\n",
    "    preds = search_hybrid(row['query'], top_k=1)\n",
    "    if target in [normalize_simple(p).strip() for p in preds]:\n",
    "        correct += 1\n",
    "print(f\"Точность Hybrid (Pretrained W2V * TF-IDF) @1: {correct / len(test_data):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9d445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
